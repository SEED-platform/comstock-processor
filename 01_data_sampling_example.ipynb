{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "\n",
    "from lib.comstock_processor import ComStockProcessor\n",
    "\n",
    "dataset_path = Path().resolve().cwd() / \"datasets\"\n",
    "print(f\"Dataset path: {dataset_path}\")\n",
    "\n",
    "data_path = dataset_path / \"comstock\"\n",
    "print(f\"Data path: {data_path}\")\n",
    "\n",
    "time_series_data_path = data_path / \"time_series_data\"\n",
    "print(f\"Time Series Data path: {time_series_data_path}\")\n",
    "\n",
    "figures_path = Path().resolve() / \"figures\"\n",
    "for path in [figures_path, time_series_data_path]:\n",
    "    if not path.exists():\n",
    "        path.mkdir(parents=True)\n",
    "print(f\"Figures path: {figures_path}\")\n",
    "\n",
    "# ignore SettingWithCopyWarning\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Allow autoreload as we develop dependent packages in parallel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ComStock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull down ComStock metadata\n",
    "\n",
    "The data will be saved into the ComStock datasets subfolder. It takes a while to run (10ish minutes) but will only download the file if it does not already exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in ComStock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and convert the data file to a dataframe.\n",
    "processor = ComStockProcessor(state=\"CA\", county_name=\"All\", building_type=\"All\", upgrade=\"0\", base_dir=data_path)\n",
    "df_all = processor.process_metadata(save_dir=data_path)\n",
    "display(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the df_all data. First, group all the buildings by building_type, county_name, and heating_fuel.\n",
    "# Set seed to 42, to make it reproducible for us all\n",
    "df_all_sample = df_all.groupby([\"in.comstock_building_type\", \"in.county_name\", \"in.heating_fuel\"]).apply(\n",
    "    lambda x: x.sample(1, random_state=42)\n",
    ")\n",
    "\n",
    "# reflatten the data\n",
    "df_all_sample = df_all_sample.reset_index(drop=True)\n",
    "display(df_all_sample)\n",
    "\n",
    "# plot the parameter spaces: in.building_type, county_name, heating fuel\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "df_all_sample.plot.scatter(x=\"in.county_name\", y=\"in.comstock_building_type\", c=\"in.sqft\", cmap=\"viridis\", ax=ax)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "df_all_sample.plot.scatter(x=\"in.heating_fuel\", y=\"in.comstock_building_type\", c=\"in.sqft\", cmap=\"viridis\", ax=ax)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# get list of IDs from the sample,\n",
    "download_timeseries_id = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplify data fields available and write simplified version to csv for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df_all[\n",
    "    [\n",
    "        \"bldg_id\",\n",
    "        \"in.sqft\",\n",
    "        \"in.state\",\n",
    "        \"in.county_name\",\n",
    "        \"in.comstock_building_type\",\n",
    "        \"in.building_subtype\",\n",
    "        \"in.heating_fuel\",\n",
    "        \"out.site_energy.total.energy_consumption_intensity\",\n",
    "        \"out.district_cooling.total.energy_consumption\",\n",
    "        \"out.district_heating.total.energy_consumption\",\n",
    "        \"out.electricity.total.energy_consumption\",\n",
    "        \"out.natural_gas.total.energy_consumption\",\n",
    "        \"out.other_fuel.total.energy_consumption\",\n",
    "        \"out.utility_bills.electricity_bill_max..usd\",\n",
    "        \"out.utility_bills.electricity_bill_mean..usd\",\n",
    "        \"out.utility_bills.electricity_bill_median..usd\",\n",
    "        \"out.utility_bills.electricity_bill_min..usd\",\n",
    "        \"out.utility_bills.electricity_bill_number_of_rates..usd\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# save to csv\n",
    "df_sub.to_csv(data_path / \"CA-All-All-0-metadata_simplified.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only download 101 buildings for sampling purposes and testing this notebook/repo\n",
    "timeseries_to_download = df_sub.head(10)\n",
    "print(f\"Downloading time series data for {len(timeseries_to_download)} buildings into {time_series_data_path}\")\n",
    "\n",
    "time_series_data_file_paths, building_ids = processor.process_building_time_series(\n",
    "    data_frame=timeseries_to_download, save_dir=time_series_data_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post process all the files and build up the combined data file, this can take about 1 minute to run (with 10 cores)\n",
    "\n",
    "# if the data file exists, then just read that, otherwise, recreate the postprocessed file.\n",
    "num_workers = max(1, multiprocessing.cpu_count() - 1)\n",
    "\n",
    "all_sampled_timeseries_filename = time_series_data_path / \"all_sampled_buildings-upgrade-0.parquet\"\n",
    "\n",
    "if all_sampled_timeseries_filename.exists():\n",
    "    print(\"Loading in the sampled timeseries data from disk, delete the file if you need to recreate it\")\n",
    "    all_ts_df = pd.read_parquet(all_sampled_timeseries_filename)\n",
    "else:\n",
    "\n",
    "    def read_and_process_file(args) -> pd.DataFrame:\n",
    "        \"\"\"Method to read and process input file so that this can be used in parallel\"\"\"\n",
    "        data_file, building_id = args\n",
    "        # only read a copy of the columns in the file that we need, extend as needed.\n",
    "        tdf = pd.read_parquet(\n",
    "            data_file,\n",
    "            columns=[\n",
    "                \"timestamp\",\n",
    "                \"out.electricity.total.energy_consumption\",\n",
    "                \"out.natural_gas.total.energy_consumption\",\n",
    "            ],\n",
    "        )\n",
    "        tdf[\"bldg_id\"] = building_id\n",
    "        return tdf\n",
    "\n",
    "    # parallel process everything\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        all_dfs = list(executor.map(read_and_process_file, zip(time_series_data_file_paths, building_ids)))\n",
    "\n",
    "    # concatenate the dataframes that were read in parallel.\n",
    "    all_ts_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    # move the \"bldg_id\" column to the beginning\n",
    "    all_ts_df = all_ts_df[[\"bldg_id\"] + [col for col in all_ts_df.columns if col != \"bldg_id\"]]\n",
    "    all_ts_df.to_parquet(all_sampled_timeseries_filename)\n",
    "\n",
    "display(all_ts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dimensions\n",
    "print(df_sub.shape)\n",
    "# show all states\n",
    "print(df_sub[\"in.state\"].unique())\n",
    "# show all the building types\n",
    "print(df_sub[\"in.comstock_building_type\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the names of the fields to a list\n",
    "fields = df_all.columns\n",
    "with open(data_path / \"comstock_metadata_fields.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(fields))\n",
    "    # write blank line at end\n",
    "    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only buildings in CA\n",
    "df_ca = df_sub[df_sub[\"in.state\"].str.contains(\"CA\")]\n",
    "print(f\"all: {df_sub.shape}\")\n",
    "print(f\"cz7: {df_ca.shape}\")\n",
    "for to_display in [\"in.building_subtype\", \"in.comstock_building_type\"]:\n",
    "    df_to_show = df_ca[to_display].value_counts()\n",
    "    df_to_show = df_to_show.reset_index()\n",
    "    display(HTML(df_to_show.to_html(index=False, border=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_counts = df_ca[\"in.county_name\"].value_counts()\n",
    "# make it a dataframe\n",
    "county_counts = county_counts.reset_index()\n",
    "# in the in.county_name remove CA, and County\n",
    "county_counts[\"in.county_name\"] = county_counts[\"in.county_name\"].str.replace(\"CA,\", \"\").str.replace(\"County\", \"\")\n",
    "# and trim the remainder\n",
    "county_counts[\"in.county_name\"] = county_counts[\"in.county_name\"].str.strip()\n",
    "\n",
    "# display the counts nicely, but with no index\n",
    "display(HTML(county_counts.to_html(index=False, border=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a built-in dataset of US counties (you need geopandas installed)\n",
    "counties_gdf = gpd.read_file(\"https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json\")\n",
    "\n",
    "# Filter for California counties (FIPS state code for CA is '06')\n",
    "california_counties = counties_gdf[counties_gdf[\"STATE\"] == \"06\"]\n",
    "\n",
    "# convert county_counts to a dictionary of format {county_name: [\"a\", \"b\", \"c\"], count: [1, 2, 3]}\n",
    "county_mapping = dict(zip(county_counts[\"in.county_name\"], county_counts[\"count\"]))\n",
    "california_counties[\"count\"] = california_counties[\"NAME\"].map(county_mapping)\n",
    "\n",
    "# Plot the map on a logscale\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "california_counties.plot(\n",
    "    column=\"count\",\n",
    "    cmap=\"Oranges\",\n",
    "    legend=True,\n",
    "    missing_kwds={\"color\": \"lightgrey\"},\n",
    "    ax=ax,\n",
    ")\n",
    "plt.title(\"Count of ComStock Buildings in CA\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# create another plot but without Los Angeles\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "california_counties[california_counties[\"NAME\"] != \"Los Angeles\"].plot(\n",
    "    column=\"count\",\n",
    "    cmap=\"Oranges\",\n",
    "    legend=True,\n",
    "    missing_kwds={\"color\": \"lightgrey\"},\n",
    "    ax=ax,\n",
    ")\n",
    "plt.title(\"Count of ComStock Buildings in CA (without LA)\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pie chart of the total energy in kwh of district cooling, district heating, electricity, natural gas\n",
    "meters = [\n",
    "    \"out.district_cooling.total.energy_consumption\",\n",
    "    \"out.district_heating.total.energy_consumption\",\n",
    "    \"out.electricity.total.energy_consumption\",\n",
    "    \"out.natural_gas.total.energy_consumption\",\n",
    "    \"out.other_fuel.total.energy_consumption\",\n",
    "]\n",
    "\n",
    "# create a new dataframe with the sum of each meter\n",
    "df_meters = df_ca[meters].sum()\n",
    "print(df_meters)\n",
    "# rename the columns to be shorter, electricity, natural gas, district heating,\n",
    "# district cooling, and other fuel\n",
    "df_meters = df_meters.rename(\n",
    "    {\n",
    "        \"out.district_cooling.total.energy_consumption\": \"District Cooling\",\n",
    "        \"out.district_heating.total.energy_consumption\": \"District Heating\",\n",
    "        \"out.electricity.total.energy_consumption\": \"Electricity\",\n",
    "        \"out.natural_gas.total.energy_consumption\": \"Natural Gas\",\n",
    "        \"out.other_fuel.total.energy_consumption\": \"Other Fuel\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# plot the meter totals as a pie chart\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "plt.pie(df_meters, labels=df_meters.index, autopct=\"%1.1f%%\")\n",
    "plt.title(\"ComStock - Energy Consumption by Meter Type\", fontsize=16)\n",
    "\n",
    "# now show the heating fuel type by count\n",
    "df_heating = df_ca[\"in.heating_fuel\"].value_counts()\n",
    "df_heating = df_heating.reset_index()\n",
    "display(HTML(df_heating.to_html(index=False, border=1)))\n",
    "# make a pie chart of count of heating fuels\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "plt.pie(df_heating[\"count\"], labels=df_heating[\"in.heating_fuel\"], autopct=\"%1.1f%%\")\n",
    "plt.title(\"ComStock - Heating Fuel Type by Count\", fontsize=16)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
